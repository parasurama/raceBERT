\documentclass[11pt]{article}

\usepackage{amsmath, mathtools, amsthm, graphicx, float, bm, csquotes}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[backend=biber,style=authoryear, sorting=nyt, citestyle=authoryear, doi=false,isbn=false,url=false]{biblatex}
\addbibresource{references.bib}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{times}
\usepackage{graphicx}
\usepackage{hyphenat}

\begin{document}

\title{raceBERT -- A Transformer-based Model for Predicting Race and Ethnicity from Names\footnote{https://github.com/parasurama/raceBERT}}
\author{Prasanna Parasurama\thanks{pparasurama@gmail.com} \\ New York University}
\date{\today}
\maketitle

\begin{abstract}
 This paper presents raceBERT -- a transformer-based model for predicting race and ethnicity from character sequences in names, and an accompanying python package.
 Using a transformer-based model trained on a U.S. Florida voter registration dataset\footnote{I sincerely thank Gaurav Sood and Suriyan Laohaprapanon for sharing their data}, the model predicts the likelihood of a name belonging to 5 U.S. census race categories (White, Black, Hispanic, Asian \& Pacific Islander, American Indian \& Alaskan Native).
 I build on \textcite{sood_predicting_2018} by replacing their LSTM model with transformer-based models (pre-trained BERT model, and a roBERTa model trained from scratch), and compare the results.
 To the best of my knowledge, raceBERT achieves state-of-the-art results in race prediction using names, with an average f1-score of 0.86 -- a 4.1\% improvement over the previous state-of-the-art, and improvements between 15-17\% for non-white names.
\end{abstract}

\section{Introduction}

Researchers studying racial disparities often do not have self-reported demographic data readily available, and must rely on proxies such as name and location to predict race (e.g. \textcite{zhang_assessing_2018,fiscella_use_2006}).
Similary, researchers studying racial discrimination are often interested in racial \textit{signal} encoded in names \parencite{bertrand_are_2004,kang_whitened_2016}. 
In hiring discrimination studies, for example, the recruiter's perception of a candidate's race (as proxied by name) becomes important, in which case it's useful to estimate the likelihood of name belonging to a particular race \parencite{parasurama_who_2020}. 
Over the years, new methods and models have been proposed to incrementally improve the accuracy of race prediction models \parencite{fiscella_use_2006,imai_improving_2016,ambekar_name-ethnicity_2009,sood_predicting_2018,xie_rethnicity_2021}.
This paper contributes to this line of research by presenting a transformer-based race and ethnicity prediction model, which, to the best of my knowledge, achieves state-of-the-art results in predictive accuracy. 

\section{Data}
The race prediction model uses the U.S. Florida voter registration dataset\footnote{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/UBIG3F} collected by \textcite{sood_predicting_2018}.
In total, there are 13,089,545 names across 5 race categories.
Table \ref{tab:race_counts} reports the counts and label for each race category.
Note that \texttt{unknown}, \texttt{other}, and \texttt{multiracial} categories are dropped, because there are too few examples.

\begin{table}[H]
 \centering
 \input{tables/race_counts.tex}
 \caption{Florida voter registration data race label and counts}
 \label{tab:race_counts}
\end{table}

\section{Models}
The main point of departure from \textcite{sood_predicting_2018} is that I replace their LSTM classifier with transformer-based models.

\subsection{Fine-tuning a pre-trained BERT model}

As a baseline, I start with a pretrained BERT\footnote{https://huggingface.co/bert-base-uncased} model for sequence classification \parencite{vaswani_attention_2017,devlin_bert_2019}.
Although BERT classification typically operates at the sentence level with sequences of tokens, it can also be used at the token level with the sequences of word pieces (i.e. sequences of characters).
First, in the preprocessing step, the name is lowercased, and the first name and the last name are concatenated by an underscore.
For example \texttt{George Smith} becomes \texttt{george\_smith}.
Then the normalized name gets tokenized into words or word pieces depending on whether the name is in the model's vocabulary. 
For example, \texttt{george\_smith} gets tokenizen into \texttt{{[[CLS], george, \_, smith, [SEP]]}}.
Here, \texttt{george} and \texttt{smith} become distinct tokens because both names are part of the vocabulary. 
If a name is not in the vocabulary, it gets tokenized into word pieces. 
For example, \texttt{satoshi} and \texttt{nakamoto} are not in the vocabulary, therefore \texttt{satoshi\_nakamoto} gets tokenized into \texttt{[[CLS], sato, \#\#shi, \_, nak, \#\#amo, \#\#to, [SEP]]}

For the training step, following \textcite{sun_how_2019}, I use the following hyperparameters: \texttt{N\_EPOCHS=4, BATCH\_SIZE\_PER\_GPU=128, LEARNING\_RATE=2e-5, WEIGHT\_DECAY=2e-5}. 
See the \href{https://wandb.ai/parasu/raceBERT-public/runs/39jr1hrc/overview}{wandb project page} for a complete list of hyperparameters and configs. 

Table \ref{tab:pretrained_race_model_performance} reports the performance of the pre-trained model on a hold-out test set, and Table \ref{tab:pretrained_race_model_performance_comparison} compares raceBERT's performance to ethnicolr's performance (Note that ethnicolr does not have \texttt{aian} as a category). 
raceBERT achieves better performance across all race categories compared to ethnicolr.
On average, there is a 4.4\% improvement in the f1-score, with the highest performance improvement coming from non-white names, with improvements as much as 18\% for black names.

\begin{table}[H]
 \centering
 \input{tables/race_performance.tex}
 \caption{raceBERT hold-out performence metrics (pre-trained language model)}
 \label{tab:pretrained_race_model_performance}
\end{table}

\begin{table}[H]
 \centering
 \input{tables/race_performance_without_aian.tex}
 \caption{raceBERT hold-out performence improvement (pre-trained language model)}
 \label{tab:pretrained_race_model_performance_comparison}
\end{table}

I repeat the training process with the Wikipedia ethnicity dataset \parencite{ambekar_name-ethnicity_2009,sood_predicting_2018} and report the results in Appendix A. 
As with the race model, raceBERT achieves an overall 5\% improvement over ethnicolr. 

\subsection{Training a model from scratch}

Although the pre-trained BERT model achieves significant performance improvements over ethnicolr, a few limitations remain.
First, with a vocabulary size of 30,000, the model is needlessly large for the task at hand. 
Most of the tokens in the vocabulary will never be used.
Second, many commonly occurring names are in the vocabulary, which raises the question of whether the model is simply memorizing names rather than learning from character sequences of names, in which case the generalizability of the model will suffer. 
To overcome these issues, I train a roBERTa model from scratch with a much smaller vocabulary size of 500 \parencite{liu_roberta_2019}. 

In the preprocessing step, the first name is lowercased, and the last name is uppercased, which are then concatenated by a space.
For example \texttt{George Smith} becomes \texttt{george SMITH}. 
In theory, the mixed lower/upper case will make it easier for the model to discriminate between first and last names.
Using the transformed names, I train a Byte Pair Encoding tokenizer with a max vocabulary size of 500 to learn the most commonly occurring character sequences, which is then used to tokenize all names.
For example, \texttt{george SMITH} gets tokenized into \texttt{[[CLS], ge, or, ge, SMITH, [SEP]]}.
Likewise, \texttt{satoshi NAKAMOTO} gets tokenized into \texttt{[[CLS], sa, t, o, sh, i, N, A, K, AM, O, T, O, [SEP]]}.

Next, I train a masked language model to learn the character sequences using the roBERTa masked language architecture \texttt{(ATTENTION\_HEADS=12, HIDDEN\_LAYERS=6)}. 
Using the weights from the language model, I initialize a roBERTa sequence classification model and train using the following hyperparameters: \texttt{N\_EPOCHS=4, BATCH\_SIZE\_PER\_GPU=128. LEARNING\_RATE=2e-5, WEIGHT\_DECAY=2e-5}.
All other configs can be found at the \href{https://wandb.ai/parasu/raceBERT-public/runs/39jr1hrc/overview}{wandb project page}.

Table \ref{race_model_performance} reports the performance of the model on a hold-out test set, and \autoref{race_model_performance_without_aian} reports the performance improvement compared to ethnicolr.
The model's performance metrics are almost the same as the pre-trained BERT model, but with a much smaller vocabulary and (theoretically) greater generalizability.
As such, I use this as the default model for the python package.

\begin{table}[H]
 \centering
 \input{tables/race_performance_bert_scratch.tex}
 \caption{raceBERT hold-out performence metrics}
 \label{race_model_performance}
\end{table}

\begin{table}[H]
 \centering
 \input{tables/race_bert_scratch_without_aian.tex}
 \caption{raceBERT hold-out performence improvements}
 \label{race_model_performance_without_aian}
\end{table}

\subsection{Code, Configs, and Resources}
All of the training code, as well as the raceBERT python package code, is on \href{https://github.com/parasurama/raceBERT}{Github}\footnote{https://github.com/parasurama/raceBERT}.
The trained models are uploaded to the \href{https://huggingface.co/pparasurama/raceBERT}{huggingface hub}\footnote{https://huggingface.co/pparasurama/raceBERT}.
All training configurations, hyperparameters, and learning curves can be found at the \href{https://wandb.ai/parasu/raceBERT-public}{wandb project page}\footnote{https://wandb.ai/parasu/raceBERT-public}.


\section{Conclusion \& Limitations}
This paper presents a new transformer-based model for predicting race from names and demonstrates the performance improvements over existing state-of-the-art models.
One limitation of this model is that it's trained on the Florida voter registration dataset, which is not necessarily representative of the U.S. population. 
As such, the accuracy scores may vary when used on datasets from U.S. regions with different demographic distributions. 
Another limitation -- one that's shared by all race prediction models -- is that the model is not 100\% accurate.
While this is admissable when studying racial disparities in the aggregate, or when estimating the likelihood of a name belonging to a particular race, it's not advisable to use this model in applications where it is important to know individual race.
 
\pagebreak

\printbibliography

\pagebreak

\section*{Appendix}
\appendix

\section{Wikipedia Ethnicity Dataset}

\begin{table}[H]
 \centering
 \input{tables/ethnicity_counts.tex}
 \label{tab:ethnicity_counts}
 \caption{Wikipedia dataset ethnicity categories and counts}
\end{table}

\begin{table}[H]
 \centering
 \scalebox{0.8}{\input{tables/ethncity_performance.tex}}
 \label{tab:ethnicty_performance}
 \caption{raceBERT hold-out performance on Wikipedia ethnicity dataset}
\end{table}

\end{document}

