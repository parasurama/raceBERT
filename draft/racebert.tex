\documentclass[11pt]{article}

\usepackage{amsmath, mathtools, amsthm, graphicx, float, bm, csquotes}
\usepackage{csquotes}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage[backend=biber,style=authoryear, sorting=nyt, citestyle=authoryear, doi=false,isbn=false,url=false]{biblatex}
\addbibresource{references.bib}
\usepackage[colorlinks=true,linkcolor=blue,citecolor=blue]{hyperref}
\usepackage{times}
\usepackage{graphicx}


\begin{document}

\title{raceBERT -- A Transformer-based Model for Predicting Race from Names\footnote{https://github.com/parasurama/raceBERT}}
\author{Prasanna Parasurama\thanks{pparasurama@gmail.com} \\ New York University}
\date{\today}
\maketitle

\section{Introduction}

This paper presents raceBERT -- a transformer-based model for predicting race from character sequences in names, and an accompanying python package.
Using a transformer-based model trained on a U.S Florida voter registration dataset\footnote{I sincerely thank Gaurav Sood and Suriyan Laohaprapanon for sharing their data}, the model predicts the likelihood of a name belonging to 5 U.S. census race categories (White, Black, Hispanic, Asian \& Pacific Islander, American Indian \& Alaskan Native).
I build on \textcite{sood_predicting_2018} by replacing their LSTM model with transformer-based models (pre-trained BERT model, and a roBERTa model trained from scratch), and compare the results.
To the best of my knowledge, raceBERT achieves state-of-the-art results in race prediction using names, with an average f1-score of 0.86 -- a 4.1\% improvement over the previous state-of-the-art -- and improvement as high as 15-17\% for non-White names.

\section{Data}
The race prediction model uses the U.S Florida voter registration dataset collected by \textcite{sood_predicting_2018}.
In total, there are 13,089,545 names across 5 race categories.
Table \ref{tab:race_counts} reports the counts and label for each race category.
Note that \texttt{unknown}, \texttt{other}, and \texttt{multiracial} categories are dropped, because there are too few examples.

\begin{table}[H]
 \centering
 \input{tables/race_counts.tex}
 \caption{Race Labels and Counts}
 \label{tab:race_counts}
\end{table}

\section{Models}
The main point of departure from \textcite{sood_predicting_2018} is that I replace their LSTM classifier with transformer-based models.

\subsection{Fine-tuning a Pre-Trained BERT Model}

As a baseline, I start with a pretrained BERT\footnote{https://huggingface.co/bert-base-uncased} model for sequence classification \parencite{vaswani_attention_2017,devlin_bert_2019}.
Although BERT classification typically operates at the sentence level with sequences of tokens, it can also be used at the token level with the sequences of word pieces (i.e. sequences of characters).
First, in the preprocessing step, the name is lowercased, and the first name and the last name are concatenated by an underscore.
For example \texttt{George Smith} becomes \texttt{george\_smith}.
Then the normalized name gets tokenized into words or word pieces depending on whether the name is in the model's vocabularyulary. 
For example, \texttt{george\_smith} gets tokenizen into \texttt{{[[CLS], george, \_, smith, [SEP]]}}.
Here, \texttt{george} and \texttt{smith} become distinct tokens because both names are part of the vocabularyulary. 
If a name is not in the vocabularyulary, it gets tokenized into word pieces. 
For example, \texttt{satoshi} and \texttt{nakamoto} are not in the vocabularyulary, therefore \texttt{satoshi\_nakamoto} gets tokenized into \texttt{[[CLS], sato, \#\#shi, \_, nak, \#\#amo, \#\#to, [SEP]]}

For the training step, following \textcite{sun_how_2019}, I use the following hyperparameters: \texttt{N\_EPOCHS=4, BATCH\_SIZE\_PER\_GPU=128, LEARNING\_RATE=2e-5, WEIGHT\_DECAY=2e-5}. 
See the \href{https://wandb.ai/parasu/raceBERT-public/runs/39jr1hrc/overview}{wandb project page} for a complete list of hyperparameters and configs. 

Table \ref{tab:pretrained_race_model_performance} reports the performance of the pre-trained model on a hold-out test set, and Table \ref{tab:pretrained_race_model_performance_comparison} compares receBERTs performance to ethnicolr's performance (Note that ethnicolr does not have \texttt{aian} as a category). 
Compared to ethnicolr, raceBERT achieves better performance across all race categories.
On average, there is a 4.4\% improvement in the f1-score, with the highest performance improvement coming from non-white names, with improvements as much as 18\% for black names.

\begin{table}[H]
 \centering
 \input{tables/race_performance.tex}
 \caption{raceBERT hold-out performence metrics (Pre-trained language model)}
 \label{tab:pretrained_race_model_performance}
\end{table}

\begin{table}[H]
 \centering
 \input{tables/race_performance_without_aian.tex}
 \caption{raceBERT hold-out performence improvement (Pre-trained language model)}
 \label{tab:pretrained_race_model_performance_comparison}
\end{table}

I repeat the training process with the Wikipedia ethnicity dataset \parencite{ambekar_name-ethnicity_2009,sood_predicting_2018} and report the results in Appendix A. 
As with the race model, raceBERT achieves an overall 5\% improvement over ethnicolr. 

\subsection{Training a Model from Scratch}

Although the pre-trained BERT model achieves significant performance improvements over ethnicolr, a few limitations remain.
First, with a vocabulary size of 30,000, the model is needlessly large for the task at hand. 
Most of the tokens in the vocabulary will never be used.
Second, many commonly occurring names are in the vocabulary, which raises the question of whether the model is simply memorizing names rather than learning from character sequences of names, in which case the generalizability of the model will suffer. 
To overcome these issues, I train a roBERTa model completely from scratch with a much smaller vocabulary size of 500 \parencite{liu_roberta_2019}. 

In the preprocessing step, the first name is lowercased, and the last name is uppercased, which are then concatenated by a space.
For example \texttt{George Smith} becomes \texttt{george SMITH}. 
In theory, the mixed lower/upper case will make it easier for the model to discriminate between first and last names.
Using the transformed names, I train a Byte Pair Encoding tokenizer with a max vocabulary size of 500 to learn the most commonly occurring character sequences, which is then used to tokenize all names.
For example, \texttt{george SMITH} gets tokenized into \texttt{[[CLS], ge, or, ge, SMITH, [SEP]]}.
Likewise, \texttt{satoshi NAKAMOTO} gets tokenized into \texttt{[[CLS], sa, t, o, sh, i, N, A, K, AM, O, T, O, [SEP]]}.

Next, I train a masked language model to learn the character sequences using the roBERTa masked language architecture \texttt{(ATTENTION\_HEADS=12, HIDDEN\_LAYERS=6)}. 
Using the weights from the language model, I initialize a roBERTa sequence classification model and train using the following hyperparameters: \texttt{N\_EPOCHS=4, BATCH\_SIZE\_PER\_GPU=128. LEARNING\_RATE=2e-5, WEIGHT\_DECAY=2e-5}.
All other configs can be found at the \href{https://wandb.ai/parasu/raceBERT-public/runs/39jr1hrc/overview}{wandb project page}.
Therefore, I use this as the default model for the python package.

Table \ref{race_model_performance} reports the performance of the model on a hold-out test set, and \autoref{race_model_performance_without_aian} reports the performance improvement compared to ethnicolr.
This model's performance metrics are almost the same as the pre-trained BERT model, but with a much smaller vocabulary and (theoretically) greater generalizability.

\begin{table}[H]
 \centering
 \input{tables/race_performance_bert_scratch.tex}
 \label{race_model_performance}
 \caption{raceBERT hold-out performence metrics}
\end{table}

\begin{table}[H]
 \centering
 \input{tables/race_bert_scratch_without_aian.tex}
 \label{race_model_performance_without_aian}
 \caption{raceBERT hold-out performence improvements}
\end{table}


\subsection{Code, Configs, and Resources}
All of the training code, as well as the raceBERT python package code, is on \href{https://github.com/parasurama/raceBERT}{Github}\footnote{https://github.com/parasurama/raceBERT}.
The trained models are uploaded to the \href{https://huggingface.co/pparasurama/raceBERT}{huggingface hub}\footnote{https://huggingface.co/pparasurama/raceBERT}.
All training configurations, hyperparameters, and learning curves can be found at the \href{https://wandb.ai/parasu/raceBERT-public}{wandb project page}\footnote{https://wandb.ai/parasu/raceBERT-public}.

\pagebreak

\printbibliography

\pagebreak

\section*{Appendix}
\appendix

\section{Wikipedia Ethnicity Dataset}

\begin{table}[H]
 \centering
 \input{tables/ethnicity_counts.tex}
 \label{tab:ethnicity_counts}
 \caption{Wikipedia dataset ethnicity categories and counts}
\end{table}

\begin{table}[H]
 \centering
 \scalebox{0.8}{\input{tables/ethncity_performance.tex}}
 \label{tab:ethnicty_performance}
 \caption{raceBERT hold-out performance on Wikipedia ethnicity dataset}
\end{table}

\end{document}

